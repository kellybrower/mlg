{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercise 2 - Graph Convolutional Networks",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kellybrower/mlg/blob/master/Exercise_2_Graph_Convolutional_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JS3SBY4fOadG",
        "colab_type": "text"
      },
      "source": [
        "![Logo](https://uploads-ssl.webflow.com/5a72b3a09e04c7000168f4de/5ce6005b22f44fde8ced717c_MD%20Horizontal.png)\n",
        "\n",
        "# Octavian.ai machine learning on graphs course\n",
        "\n",
        "Welcome to our summer course on graph ML.\n",
        "\n",
        "This course is primarily exercise based - you'll learn through reading and writing code, and answering the questions throughout these exercises.\n",
        "\n",
        "[Join our Discord](https://discord.gg/a2Z82Te) to chat with fellow enthusiasts about this exercise and give us feedback to direct the next one.\n",
        "\n",
        "## Exercise 2, graph convolutional networks\n",
        "In this exercise, you will learn how to classify nodes in a graph. We'll do this by creating a graph network that passes messages along the edges of the graph.\n",
        "\n",
        "This technique is very versatile and with creativity can be applied to a wide range of graph problems.\n",
        "\n",
        "### Dataset\n",
        "\n",
        "We'll work with the popular [Cora](https://relational.fit.cvut.cz/dataset/CORA) dataset. Using a well known dataset makes this exercise easier as there are lots of existing solutions to look at if you run into trouble. Also, we know a solution is possible, which is not always the case in ML research.\n",
        "\n",
        "Cora is a academic paper citation graph. Its nodes are papers and the edges are citations between them. Each paper also comes with a set of mentioned topics, which we will use to help increase the network's classificiation accuracy.\n",
        "\n",
        "Each node has a classification label, which we will train our network to output:\n",
        "*\t\tCase_Based\n",
        "*\t\tGenetic_Algorithms\n",
        "*\t\tNeural_Networks\n",
        "*\t\tProbabilistic_Methods\n",
        "*\t\tReinforcement_Learning\n",
        "*\t\tRule_Learning\n",
        "*\t\tTheory\n",
        "\n",
        "Some statistics from the Relational Dataset Repository:\n",
        ">The Cora dataset consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words.\n",
        "\n",
        "### Introduction to Graph Convolutional Networks (GCN)\n",
        "\n",
        "A graph convolutional network (GCN) is a machine learning technique for graphs. In a GCN each node has an initial state and directed edge weights:\n",
        "\n",
        "![Illustration of initial graph state](https://uploads-ssl.webflow.com/5a72b3a09e04c7000168f4de/5d4f208e2108a9472bcd5f70_Graph%20illustrations%20(1).png)\n",
        "\n",
        "This initial state could be some known information about a node (in our case, the keywords of the paper), or it could be a fixed/learned/random value.\n",
        "\n",
        "Then a number (determined by the engineer) of GCN layers are executed. In each GCN layer:\n",
        "- Propagate each node's state to its neighbors along the graph's edges (a weighted sum by edge weight)\n",
        "- Then apply the same dense layer *W* to every node, with an optional activation function *σ*\n",
        "\n",
        "![illustration of propagation](https://uploads-ssl.webflow.com/5a72b3a09e04c7000168f4de/5d4f208b8ab9f5fc43094ed8_Graph%20illustrations.png)\n",
        "\n",
        "These two steps can be thought of as the eqivalent of a dense layer in a standard feedforward network, you get to design the architecture by making decisions such as:\n",
        "- How many of these GCN layers to stack\n",
        "- What size should the output node state be after each layer?\n",
        "- What activation function should each GCN layer use?\n",
        "- What regularisation should be applied?\n",
        "\n",
        "The result of this network is a state for each node in the graph. \n",
        "\n",
        "To use these node states as node classifications (as we shall in this exercise), softmax can be used as the final activation function. [This generates a probability-distribution-like vector for each node](https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/softmax).\n",
        "\n",
        "### Theoretical background\n",
        "\n",
        "Thomas Kipf has published [really excellent articles](https://tkipf.github.io/graph-convolutional-networks/) about this area of technology, and this tutorial is based off of his basic network structure [outlined here](https://tkipf.github.io/graph-convolutional-networks/).\n",
        "\n",
        "You're encouraged to read Thomas's articles to get the full background on this technique. This exercise focuses on the application of it, as opposed to the background and theory.\n",
        "\n",
        "This area of technology is still in its infancy; The capabilties of GCN have not been fully charted. Whilst working on this exercise, embrace a healthy relish for research and the unknown!\n",
        "\n",
        "### Exercise structure\n",
        "\n",
        "In this exercise you will create a fully functioning graph convolutional network. \n",
        "\n",
        "The exercise is a series of empty functions that you will fill out according to the instructions. There are then a series of unit tests to verify that your code works according to plan.\n",
        "\n",
        "# The exercise\n",
        "\n",
        "## Library setup\n",
        "\n",
        "We'll write our code in Tensorflow 2.0 and Keras. We'll also use Numpy and Scipy for some of the initial data manipulation. Let's load up all the libraries we'll need:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESFlVsylONbD",
        "colab_type": "code",
        "outputId": "7ce3cde7-11a2-4993-d852-ec3e8ec9f7a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Import all the libraries we need\n",
        "!pip uninstall -q -y tensorflow\n",
        "!pip uninstall -q -y grpcio\n",
        "\n",
        "!pip install -q tensorflow==2.0.0\n",
        "!pip install -q grpcio==1.24.3\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, backend as bk\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "import scipy.sparse as sp\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "import collections\n",
        "from collections import namedtuple\n",
        "import unittest\n",
        "import os\n",
        "import sys\n",
        "\n",
        "print(\"TensorFlow version: \", tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow version:  2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0QUSDq5pgUB",
        "colab_type": "text"
      },
      "source": [
        "## Data loading\n",
        "\n",
        "We're going to use the data as prepared in Thomas Kipf's TensorFlow GCN codebase. It's been neatly pickled so that we can easily load it from disk. The following code will download the data, the TensorFlow codebase, and load the data into memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUQDQ_HTuoIT",
        "colab_type": "code",
        "outputId": "b0dbe9ff-fdb8-4ebc-b697-a15e76d459af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# Thanks to Thomas Kipf for sharing this and also for all his work\n",
        "# researching and publicizing GCNs\n",
        "!git clone https://github.com/tkipf/gcn.git\n",
        "\n",
        "# Add the GCN repo to the import path\n",
        "sys.path.append('/content/gcn')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gcn'...\n",
            "remote: Enumerating objects: 143, done.\u001b[K\n",
            "remote: Total 143 (delta 0), reused 0 (delta 0), pack-reused 143\u001b[K\n",
            "Receiving objects: 100% (143/143), 5.08 MiB | 18.38 MiB/s, done.\n",
            "Resolving deltas: 100% (70/70), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKjWq_p1piBj",
        "colab_type": "code",
        "outputId": "fb05a628-c755-4c29-b5da-279cb12c2f96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "# Load the data\n",
        "%cd -q /content/gcn/gcn\n",
        "from gcn.utils import load_data\n",
        "\n",
        "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data('cora')\n",
        "print(\"Loaded\",len(y_train),\"nodes\")\n",
        "print()\n",
        "print(\"-- Data format --\")\n",
        "print(\"Adj:       \", adj.shape,             type(adj), \"number of indices\", len(adj.indices))\n",
        "print(\"y_train:   \", y_train.shape, \"\\t\",   type(y_train))\n",
        "print(\"train_mask:\", train_mask.shape,\"\\t\", type(train_mask))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 2708 nodes\n",
            "\n",
            "-- Data format --\n",
            "Adj:        (2708, 2708) <class 'scipy.sparse.csr.csr_matrix'> number of indices 10556\n",
            "y_train:    (2708, 7) \t <class 'numpy.ndarray'>\n",
            "train_mask: (2708,) \t <class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx00AWImwKxl",
        "colab_type": "text"
      },
      "source": [
        "## Data preparation\n",
        "\n",
        "The adjacency matrix is a representation of all the edges in the graph. For each node *i* and *j* in the graph, if they have an edge from *i* to *j* then `adj[i][j] == 1.0`, else `0.0`. Here's an example:\n",
        "\n",
        "![Adjacency matrix format](https://uploads-ssl.webflow.com/5a72b3a09e04c7000168f4de/5d508c94a4a3707d208a52d3_Matrix%20illustration.png)\n",
        "\n",
        "We need to do a little preparation of the adjacency matrix so it'll work well with our GCN layers.\n",
        "\n",
        "First, we need to add self-edges. This allows each node to propagate state back to itself, which allows nodes to retain information.\n",
        "\n",
        "Secondly, the adjacency matrix needs to be [normalised](). This ensures that when node states are propagated during the GCN layer, the size of the result is the average of the neighbors (instead of an ever-increasing sum, depending on the number of incoming edges a node has).\n",
        "\n",
        "The [degree matrix](https://en.wikipedia.org/wiki/Degree_matrix) measures how many edges each node has. We'll essentially divide the adjacency matrix by that so it does not grow the size of the node states each iteration.\n",
        "\n",
        "We'll use the following symmetric normalization technique, which has been noted for its [useful dynamics](https://arxiv.org/abs/1609.02907):\n",
        "\n",
        "<img src=\"https://uploads-ssl.webflow.com/5a72b3a09e04c7000168f4de/5d508ecc3691c96989e3d4f0_ex2%20sym%20normalize.png\" width=\"400px\"/>\n",
        "\n",
        "Note that the adjacency matrix is [sparse](https://en.wikipedia.org/wiki/Sparse_matrix): instead of storing every value in a *2708 × 2708 × sizeof(float)* memory matrix (hint: that's a lot of memory!), just the non-zero values are stored as a list. Its internal structure is a [list of tuples](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html), each with a numeric value and a coordinate of where in the matrix that value appears.\n",
        "\n",
        "Storing it sparsely greatly reduces the memory footprint of the matrix, making it easier to store, move and process. It does have one downside however: TensorFlow's library function support for sparse matrices is still quite limited.\n",
        "\n",
        "Numpy and Scipy have a richer set of functions for manipulating sparse matrices, so we'll use those to normalize the adjacency matrix prior to feeding it into Tensorflow-Keras.\n",
        "\n",
        "The following functions will be helpful:\n",
        "- `adj.sum(axis)` to get the sum along an axis (hint: the initial adjacency matrix has 1s where there are edges, so summing it along an axis generates the degrees as a vector - which you could diagonalize into a matrix)\n",
        "- `np.power(vector, power)` to square / square-root a matrix\n",
        "- `m[np.isinf(m)] = 0.0` lets you trip infinite values from a matrix, which could occur if you square-root zero\n",
        "- `sp.diags(m)` to get the [diagonal](https://en.wikipedia.org/wiki/Diagonal_matrix) of a matrix\n",
        "- `m.dot(n)` to multiply two matrices together\n",
        "- `sp.eye(m.shape[0])` to get an [identity matrix](https://en.wikipedia.org/wiki/Identity_matrix) the same size as a square matrix `m` - useful for adding self-edges\n",
        "- `m.astype(dtype)` To cast your result to the desired type\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FEg5eu51c3O",
        "colab_type": "code",
        "outputId": "0fb11e74-d0a3-49b1-ec8e-0334d4631c83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "# Takes a scipy csr matrix, returns a csr matrix\n",
        "def sym_normalize_matrix(adj, dtype=np.float32):\n",
        "  '''\n",
        "  Parameters:\n",
        "    adj: The matrix to normalize\n",
        "    dtype: The desired output dtype (e.g. the type of the values in the sparse matrix)\n",
        "  '''\n",
        "  # Apply the matrix normalization D^(-1/2) x A x D^(-1/2) where D is the degree and A the adjacency\n",
        "  \n",
        "  # --- WRITE CODE HERE ---\n",
        "\n",
        "\n",
        "# Takes a scipy csr matrix, returns a csr matrix\n",
        "def prepare_adj_matrix(adj):\n",
        "  # Add self-edges to adj, then apply sym_normaliize_matrix to the result\n",
        "  \n",
        "  # --- WRITE CODE HERE ---\n",
        "\n",
        "\n",
        "prepared_adj = prepare_adj_matrix(adj)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Tests to validate your code\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "class TestAdjNormalization(unittest.TestCase):\n",
        "\n",
        "  def assert_csr_close(self, result, expected):\n",
        "    np.testing.assert_allclose(result.indices, expected.indices, err_msg=\"Indices mismatch\")\n",
        "    np.testing.assert_allclose(result.data,  expected.data, err_msg=\"Values mismatch\")\n",
        "    self.assertEqual(result.shape, expected.shape, \"Shape mismatch\")\n",
        "  \n",
        "  def test_prepare(self):\n",
        "    with unittest.mock.patch('__main__.sym_normalize_matrix') as mock_norm:\n",
        "      test_value = sp.csr.csr_matrix(np.array([[0,1],[0,0]], np.float32))\n",
        "      test_value_exp = sp.csr.csr_matrix(np.array([[1,1],[0,1]], np.float32))\n",
        "\n",
        "      result = prepare_adj_matrix(test_value)\n",
        "\n",
        "      assert mock_norm.call_args is not None, \"sym_normalize_matrix should be called\"\n",
        "      args, kwargs = mock_norm.call_args\n",
        "      self.assert_csr_close(args[0], test_value_exp)\n",
        "\n",
        "      print(\"test_prepare success!\")\n",
        "\n",
        "  def test_normalization(self):\n",
        "    \n",
        "    adj = sp.csr_matrix(np.array([\n",
        "        [1.0, 1.0, 0.0],\n",
        "        [1.0, 1.0, 1.0],\n",
        "        [0.0, 0.0, 1.0]\n",
        "    ]))\n",
        "       \n",
        "    result = sym_normalize_matrix(adj, np.float32)\n",
        "    \n",
        "    expected_indices = np.array([[0, 0], [1, 0],      [0, 1],     [1, 1],     [2, 1],     [2, 2]])\n",
        "    expected_values  = np.array([ 0.5,    0.40824829,  0.40824829, 0.33333333, 0.57735027, 1.   ])\n",
        "    expected_shape   = (3, 3)\n",
        "\n",
        "    expected = sp.csr_matrix(np.array([\n",
        "      [0.5, 0.40824829, 0.0],\n",
        "      [0.40824829, 0.33333333, 0.57735027],\n",
        "      [0.0, 0.0, 1.0]\n",
        "    ]))\n",
        "    \n",
        "    self.assert_csr_close(result, expected)\n",
        "    assert result.dtype == np.float32, \"Result of sym_normalize_matrix should have dtype float32, got \" + str(result.dtype)\n",
        "    \n",
        "    print(\"test_normalization success!\")\n",
        "\n",
        "TestAdjNormalization().test_prepare()\n",
        "TestAdjNormalization().test_normalization()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-6240ab6a4b59>\"\u001b[0;36m, line \u001b[0;32m19\u001b[0m\n\u001b[0;31m    prepared_adj = prepare_adj_matrix(adj)\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGUjyl7ZuqRV",
        "colab_type": "text"
      },
      "source": [
        "## Keras layer\n",
        "\n",
        "Now for the exciting part, let's build a GCN layer in Keras that we can use to construct a graph convolutional network.\n",
        "\n",
        "The layer takes the *node_state* as the incoming tensor, then transforms that into a new *node_state*. It performs the following operations (many akin to a standard Dense layer):\n",
        "\n",
        "1.   [Dropout](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/)\n",
        "2.   Graph convolution (i.e. multiplying the node state by the weights matrix)\n",
        "3.   Graph propagation (i.e. multiplying the adjacency matrix by the node state)\n",
        "4.   [Activation function](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6)\n",
        "\n",
        "#### Graph convolution\n",
        "This is convolution in the sense that the same parameters are being applied to each node state. This is in the form of a shared matrix, which transforms each node state just as a [dense layer](https://towardsdatascience.com/building-neural-network-from-scratch-9c88535bf8e9) would transform the activations in a feed-forward network.\n",
        "\n",
        "#### Implementation details\n",
        "The layer takes the adjacency matrix (the sparse matrix representing graph connectivity) as a parameter in its constructor. The adjacency matrix does not change during training or testing as our graph is static.\n",
        "\n",
        "I've provided the main scaffold for the layer, initialising the weights and constructing the object.\n",
        "\n",
        "#### Your work is to fill out the **call** method of the class *GCNLayer*. I've included comments, and test cases to verify your implementation.\n",
        "\n",
        "Useful functions:\n",
        "- `tf.sparse.sparse_dense_matmul` to multiply a sparse tensor by a dense tensor\n",
        "- `tf.matmul` to multiply a dense tensor by a dense tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9SuJ4FCw0Ca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GCNLayer(keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, adjacency, units, activation=tf.identity, dropout=0.0, l2=0.0, dtype=tf.float32, name=None):\n",
        "    '''\n",
        "    Params:\n",
        "      Adjacency: a tf.SparseTensor adjacency matrix\n",
        "      Units: The number of output units per node state\n",
        "      Activation: The activation function to apply to the node states\n",
        "      Dropout: The amount of dropout (0.0 being none, 1.0 being all units) to apply\n",
        "      l2: The amount of L2 regularisation to apply\n",
        "      dtype: The type of values in the tensors this layer will transform\n",
        "      name: The name of this layer \n",
        "    '''\n",
        "    super(GCNLayer, self).__init__(dtype=dtype, name=name)\n",
        "    \n",
        "    self.adjacency = adjacency\n",
        "    self.units = units\n",
        "    self.activation = activation\n",
        "    self.dropout = dropout\n",
        "    self.l2 = l2\n",
        "    \n",
        "    assert isinstance(adjacency, tf.SparseTensor), \"Adjacency matrix should be a SparseTensor\"\n",
        "    assert adjacency.dtype == self.dtype, \"Adjacency matrix not expected dtype, got \" + str(adjacency.dtype) + \" expected \" + str(self.dtype)\n",
        "    \n",
        "  def build(self, input_shape):\n",
        "    '''\n",
        "    This method is called during the initial compilation of our model. Its \n",
        "    primary job is to initialize the weights for this layer.\n",
        "\n",
        "    Params:\n",
        "      input_shape: this is the shape of the input to the layer, in our case an \n",
        "                   array of (NUMBER_NODES, NODE_STATE_SIZE)\n",
        "\n",
        "    We build one weight, w, which will be applied to each node_state. We initialize\n",
        "    it from the uniform distribution, scaled by the size of the matrix. We apply\n",
        "    l2 loss to regularize the matrix\n",
        "    '''\n",
        "    self.w = self.add_weight(\n",
        "      shape=(input_shape[1], self.units),\n",
        "      dtype=self.dtype,\n",
        "      initializer='glorot_uniform',\n",
        "      regularizer=keras.regularizers.l2(self.l2)\n",
        "    )\n",
        "  \n",
        "    \n",
        "  def call(self, node_state):\n",
        "    '''\n",
        "    This method is called to apply the layer to an incoming tensor. This is the\n",
        "    real meat of the model.\n",
        "\n",
        "    Params:\n",
        "      node_state: The tf.Tensor of node states.  Shape (NUMBER_NODES, NODE_STATE_SIZE)\n",
        "\n",
        "    Returns: The transformed node state tf.Tensor\n",
        "    '''\n",
        "    \n",
        "    assert isinstance(node_state, tf.Tensor), \"Layer input should be a Tensor, got \" + str(type(node_state))\n",
        "    assert node_state.dtype == self.dtype, \"Input to layer \" + str(self.name) + \" wrong dtype, got \" + str(node_state.dtype) + \" expected \" + str(self.dtype)\n",
        "    tf.debugging.check_numerics(node_state, \"Input to layer \" + str(self.name) + \" has numerical instability\")\n",
        "\n",
        "    # Apply dropout to the node_state, using the self.dropout as the factor\n",
        "    \n",
        "    # --- WRITE CODE HERE ---\n",
        "   \n",
        "    # Apply the node convolution: This means to matrix multiply each node_state by our learned parameters `self.w`\n",
        "    \n",
        "    # --- WRITE CODE HERE ---\n",
        "\n",
        "    # Apply the graph propagation: This means to multiply the\n",
        "    # normalized adjacency matrix by the node state\n",
        "    # You can do this as a single sparse_dense_matmul()\n",
        "    \n",
        "    # --- WRITE CODE HERE ---\n",
        "\n",
        "    # Apply the activation function `self.activation`\n",
        "    \n",
        "    # --- WRITE CODE HERE ---\n",
        "\n",
        "    tf.debugging.check_numerics(node_state, \"Output of layer \" + str(self.name) + \" has numerical instability\")\n",
        "\n",
        "    return node_state\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Like good engineers, let's validate our code works!\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "class TestLayer(unittest.TestCase):\n",
        "\n",
        "  def test_propagate_node_state(self):\n",
        "\n",
        "    sparse_adj = tf.SparseTensor([(0,1), (1,0)], np.array([1.0, 1.0], np.float32), (3, 3))\n",
        "    l = GCNLayer(sparse_adj, units=2, dropout=0.0)\n",
        "    l.build((3, 2,))\n",
        "\n",
        "    weights = tf.constant([\n",
        "      [1.0, 0.0],\n",
        "      [0.0, 1.0]\n",
        "    ])\n",
        "    l.set_weights([weights])\n",
        "\n",
        "    node_state = tf.constant([\n",
        "        [1.0, 0.0],\n",
        "        [0.0, 0.0],\n",
        "        [0.0, 0.0]\n",
        "    ], tf.float32)\n",
        "    \n",
        "    result = l.call(node_state)\n",
        "    \n",
        "    expected_result = [\n",
        "        [0.0, 0.0],\n",
        "        [1.0, 0.0],\n",
        "        [0.0, 0.0]\n",
        "    ]\n",
        "    \n",
        "    np.testing.assert_array_equal(result, expected_result)\n",
        "    \n",
        "    print(\"test_propagate_node_state Success!\")\n",
        "    \n",
        "    \n",
        "  def test_apply_convolution(self):\n",
        "    \n",
        "    node_state = tf.constant([\n",
        "        [ 1.0, 0.0],\n",
        "        [-1.0, 0.0],\n",
        "        [ 0.0, 0.0]\n",
        "    ], tf.float32)\n",
        "\n",
        "    sparse_adj = tf.SparseTensor([(0,0), (1,1), (2,2)], np.array([1.0, 1.0, 1.0], np.float32), (3, 3))\n",
        "    l = GCNLayer(sparse_adj, units=2, dropout=0.0)\n",
        "    l.build((3, 2,))\n",
        "    \n",
        "    weights = tf.constant([\n",
        "        [0.0, 1.0],\n",
        "        [1.0, 0.0]\n",
        "    ])\n",
        "    l.set_weights([weights])\n",
        "    \n",
        "    result = l.call(node_state)\n",
        "    \n",
        "    expected_result = [\n",
        "        [0.0,  1.0],\n",
        "        [0.0, -1.0],\n",
        "        [0.0,  0.0]\n",
        "    ]\n",
        "    \n",
        "    np.testing.assert_array_equal(result, expected_result)\n",
        "    \n",
        "    print(\"test_apply_convolution Success!\")\n",
        "    \n",
        "  def test_layer(self):\n",
        "    \n",
        "    sparse_adj = tf.SparseTensor([(0,1), (1,0)], np.array([1.0, 1.0], np.float32), (3, 3))\n",
        "    \n",
        "    l = GCNLayer(sparse_adj, units=2, activation=keras.backend.relu, dropout=0.0001)\n",
        "    l.build((4, 2,))\n",
        "    \n",
        "    weights = tf.constant([\n",
        "      [0.0, -1.0],\n",
        "      [1.0, 0.0]\n",
        "    ])\n",
        "    l.set_weights([weights])\n",
        "    \n",
        "    node_state = tf.constant([\n",
        "        [-1.0, 0.0],\n",
        "        [ 1.0, 0.0],\n",
        "        [ 0.0, 0.0]\n",
        "    ], tf.float32)\n",
        "    \n",
        "    result = l.call(node_state)\n",
        "    \n",
        "    expected_result = [\n",
        "        [0.0, 0.0],\n",
        "        [0.0, 1.0],\n",
        "        [0.0, 0.0]\n",
        "    ]\n",
        "    \n",
        "    np.testing.assert_allclose(result, expected_result, rtol=1e-03)\n",
        "    \n",
        "    print(\"test_layer Success!\")\n",
        "\n",
        "\n",
        "t = TestLayer()\n",
        "t.test_propagate_node_state()\n",
        "t.test_apply_convolution()\n",
        "t.test_layer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYRaDxMY-9hH",
        "colab_type": "text"
      },
      "source": [
        "## Keras model\n",
        "\n",
        "Now we have a working GCN layer, let's put it to work in a Keras model. Keras provides a simple interface for doing what we want to do, the Sequential model\n",
        "format. It stacks multiple layers linearly, passing the output of one as the input to the next.\n",
        "\n",
        "We're going to build the following network architecture - two layers, with the following parameters:\n",
        "\n",
        "1.   Output units = 16, activation = relu\n",
        "2.   Output units = number of different labels (7), activation = softmax\n",
        "\n",
        "We'll give each layer our prepared adjacency matrix from earlier.\n",
        "\n",
        "I've provided hyper-parameters for [L2](https://developers.google.com/machine-learning/glossary/#L2_regularization) and [dropout](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ragAHoJ3_8Ts",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "L2_FACTOR = 5e-3\n",
        "DROPOUT_FACTOR = 0.5\n",
        "\n",
        "NODE_COUNT   = adj.shape[0]\n",
        "NUM_CLASSES  = 7\n",
        "CLASS_LABELS = list(range(NUM_CLASSES))\n",
        "\n",
        "# Transform our Scipy CSR matrix into a tensorflow SparseTensor\n",
        "coo = prepared_adj.tocoo()\n",
        "indices = np.array(list(zip(coo.row, coo.col)))\n",
        "tf_adj = tf.SparseTensor(indices=indices, values=tf.cast(prepared_adj.data, tf.float32), dense_shape=prepared_adj.shape)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# The model\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "  # --- WRITE CODE HERE ---\n",
        "])\n",
        "\n",
        "# ------------------------------------------------------------------------------\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vr2nUPxkAxiB",
        "colab_type": "text"
      },
      "source": [
        "### Train the model\n",
        "\n",
        "Finally, let's train the model. Keras makes this simple for us, with one call to `compile` then `fit`.\n",
        "\n",
        "I've provided the optimizer and loss functions - they're a fairly common setup, using [Adam](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/) as the optimizer and [categorical cross-entropy](https://gombru.github.io/2018/05/23/cross_entropy_loss/) for the loss calculation.\n",
        "\n",
        "I've provided a helper class to monitor accuracy for us, which I've wired up in the metrics (because of how our data is structured, the normal Keras metrics won't correctly measure accuracy. We need to apply our label mask in both the loss and accuracy calculations, the custom metric below will do this).\n",
        "\n",
        "I've also provided a few graphs - you should try adding more graphs to see the inner workings of the model. Graphing is a valuable research skill."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYw5XEthB9rr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Metric to measure accuracy of our model, optionally by class.\n",
        "\n",
        "# This is needed as the way we're treating our data doesn't fit the Keras\n",
        "# metrics. In a normal ML flow you have seperate lists of examples (e.g. input \n",
        "# and expected label) for training, testing and validation. You feed one of these\n",
        "# lists in to your training/evaluation loop and measure the model's loss and\n",
        "# accuracy on those examples.\n",
        "\n",
        "# Because our network is a graph, the input to the network is a tensor of node states\n",
        "# and we need to pass the entire set of node states in for the adjacency matrix\n",
        "# to match the shape of the node state matrix. We cannot just pass in the training\n",
        "# nodes and their labels.\n",
        "\n",
        "# Therefore instead we mask the labels output by the network to just the training\n",
        "# or testing set of labels, and measure their accuracy. Keras's sample_weight\n",
        "# mechanism doesn't get applied to accuracy metrics or during testing, therefore\n",
        "# I've implemented our own metric.\n",
        "\n",
        "# This metric has one additional optional feature: It will calculate accuracy for \n",
        "# a single class.\n",
        "\n",
        "# It's important to watch the accuracy by each class label to check the network\n",
        "# is discriminating between them. If a network is struggling to train, one common\n",
        "# failure case is it predicts the same class for all labels as an easy way to \n",
        "# decrease loss. This often presents itself as a train accuracy of 100%/NUM_CLASSES.\n",
        "\n",
        "# By watching individual class acurracies, we can see if the network is learning\n",
        "# to predict each class, or sacrificing some/all classes for one class.\n",
        "\n",
        "class AccuracyByClass(keras.metrics.Metric):\n",
        "  def __init__(self, name, dtype, class_label=None, sample_weight=None, y_true=None):\n",
        "    ''' \n",
        "    Parameters:\n",
        "      name: The name of this metric\n",
        "      dtype: The type of the data being measured\n",
        "      class_label: (Optional) If you supply this, the accuracy will be measured for just that class. Otherwise overall accuracy is measured\n",
        "      sample_weight: The mask you want to apply to the labels\n",
        "      y_true: (Optional) The correct values for the labels output by the network\n",
        "\n",
        "    '''\n",
        "    super().__init__(name, dtype)\n",
        "    \n",
        "    self.class_label = class_label\n",
        "    self.sample_weight = tf.cast(sample_weight, dtype)\n",
        "    self.y_true = y_true\n",
        "\n",
        "    self.correct = tf.Variable(initial_value=0.0, dtype=self._dtype, trainable=False, name='correct_'+str(class_label))\n",
        "    self.total   = tf.Variable(initial_value=0.0, dtype=self._dtype, trainable=False, name='total_'+str(class_label)) \n",
        "    \n",
        "  def reset_states(self):\n",
        "    self.correct.assign(0)\n",
        "    self.total.assign(0)\n",
        "\n",
        "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "    # Note that Keras doesn't pass sample_weight into its metrics\n",
        "    # during testing\n",
        "\n",
        "    if self.y_true is not None:\n",
        "      y_true = self.y_true\n",
        "\n",
        "    y_true_classes = tf.argmax(y_true, axis=-1)\n",
        "    y_pred_classes = tf.argmax(y_pred, axis=-1)\n",
        "    correct = tf.equal(y_pred_classes, y_true_classes)\n",
        "\n",
        "    # Create mask\n",
        "    if self.class_label is not None:\n",
        "      mask = tf.cast(tf.equal(y_true_classes, self.class_label), self._dtype)\n",
        "    else:\n",
        "      mask = tf.ones(tf.shape(y_true_classes), self._dtype, 'mask_class_true_ones')\n",
        "    \n",
        "    sample_weight = self.sample_weight\n",
        "\n",
        "    if sample_weight is not None:\n",
        "      mask *= sample_weight\n",
        "\n",
        "    # Apply mask\n",
        "    masked_total_count = tf.reduce_sum(mask)\n",
        "    self.total.assign_add(masked_total_count)\n",
        "\n",
        "    masked_correct = mask * tf.cast(correct, self._dtype)\n",
        "    masked_correct_count = tf.reduce_sum(masked_correct)\n",
        "    self.correct.assign_add(masked_correct_count)\n",
        "  \n",
        "    return self.result()\n",
        "      \n",
        "\n",
        "  def result(self):\n",
        "    return tf.math.divide_no_nan(self.correct, self.total)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Imjm5o2XBsHs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCH_COUNT   = 200 # Determined by experiment\n",
        "LEARNING_RATE = 0.001 # Learning rate determined through experimentation\n",
        "\n",
        "# Loss function and optimizer\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE) \n",
        "\n",
        "metrics = [\n",
        "  AccuracyByClass(\"train_accuracy\", tf.float32, sample_weight=train_mask, y_true=y_train),\n",
        "  AccuracyByClass(\"test_accuracy\",  tf.float32, sample_weight=test_mask,  y_true=y_test),\n",
        "  AccuracyByClass(\"val_accuracy\",   tf.float32, sample_weight=val_mask,   y_true=y_val)\n",
        "]\n",
        "\n",
        "for label in CLASS_LABELS:\n",
        "  metrics.append(AccuracyByClass(\"train_class_acc_\"+str(label), tf.float32, label, sample_weight=train_mask, y_true=y_train))\n",
        "  metrics.append(AccuracyByClass(\"test_class_acc_\"+str(label),  tf.float32, label, sample_weight=test_mask,  y_true=y_test))\n",
        "\n",
        "# Fix the random seeds prior to compiling and training the model - this helps make\n",
        "# results reproduceable\n",
        "np.random.seed(13)\n",
        "tf.random.set_seed(13)\n",
        "\n",
        "# Generate weights, setup optimizer and loss function:\n",
        "model.compile(optimizer, loss=loss_fn, metrics=metrics)\n",
        "\n",
        "initial_state = features.todense()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    initial_state, \n",
        "    y_train, \n",
        "    sample_weight=tf.cast(train_mask, tf.float32), # This will be used in loss calculations\n",
        "    validation_data=(initial_state, y_val, val_mask),\n",
        "    epochs=EPOCH_COUNT, \n",
        "    batch_size=NODE_COUNT, # This is unusual in ML - since our adjacency matrix is the whole graph, we want to feed in the whole node_state array in each training step\n",
        "    verbose=0,\n",
        "    shuffle=False # Do not shuffle the order of our input data, since its order matches up to the adjacency matrix\n",
        ")\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqikApUyF_HE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Display graphs of how the model performed\n",
        "from scipy.ndimage.filters import gaussian_filter1d\n",
        "\n",
        "def smooth(key):\n",
        "  return gaussian_filter1d(history.history[key], sigma=2)\n",
        "\n",
        "print(\"Final loss: \", history.history['loss'][-1])\n",
        "print(\"Final training accuracy: \", round(history.history['train_accuracy'][-1]*100), \"%\")\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [15, 10]\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title(\"Training loss\")\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(smooth('train_accuracy'))\n",
        "plt.title(\"Training accuracy\")\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot(smooth('val_accuracy'))\n",
        "plt.title(\"Validation accuracy\")\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "for label in CLASS_LABELS:\n",
        "  plt.plot(smooth(\"train_class_acc_\"+str(label)))\n",
        "plt.title(\"Training accuracy by class\")\n",
        "plt.ylabel('Class accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['class_'+str(label) for label in CLASS_LABELS], loc='lower right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyPF1j5JGUrt",
        "colab_type": "text"
      },
      "source": [
        "## Test the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxJyhEQfGO-u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Apply the trained model to our test data (that is, the held-out node labels)\n",
        "# and measure how it performs\n",
        "results = model.evaluate(\n",
        "    initial_state, \n",
        "    y_test, \n",
        "    steps=1, \n",
        "    batch_size=NODE_COUNT, \n",
        "    verbose=0)\n",
        "\n",
        "# Get test metrics\n",
        "results_dict = dict(zip(model.metrics_names, results))\n",
        "for name in model.metrics_names:\n",
        "  if \"test\" not in name:\n",
        "    del results_dict[name]\n",
        "\n",
        "# Add train accuracy from the earlier training history\n",
        "results_dict[\"train_accuracy\"] = history.history['train_accuracy'][-1]\n",
        "\n",
        "results_keys = list(results_dict.keys())\n",
        "results_values = [results_dict[key] for key in results_keys]\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [15, 6]\n",
        "\n",
        "# Display a bar chart\n",
        "y_pos = np.arange(len(results_dict))\n",
        "plt.barh(y_pos, results_values, align='center', alpha=0.5)\n",
        "plt.yticks(y_pos, results_keys)\n",
        "plt.ylabel('Percentage')\n",
        "plt.title('Evaluation')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjlBnqU30QiX",
        "colab_type": "text"
      },
      "source": [
        "## Exercise questions and next steps\n",
        "\n",
        "\n",
        "[Add your answers and discoveries to the answer document](https://docs.google.com/document/d/1QdAEOYnJ5AwFczNQZk5ZZo1Ng7_shn33GzQPdumnsvI/edit?usp=sharing)\n",
        "\n",
        "\n",
        "- What train and test accuracy did you get?\n",
        "\n",
        "- What modifications did you try to the network? How did they perform?\n",
        "\n",
        "- Are there other graphs you find it useful to produce?\n",
        "\n",
        "- What is the theoretical capabilities of this network? Do we need other methods of graph machine learning?\n",
        "\n",
        "- Find an example of a Graph Convolutional Network being used in industry, link to it here and provide a summary\n",
        "\n",
        "- How would you scale this network to a larger graph? What challenges might you encounter?\n",
        "\n",
        "- Imagine you’re applying this method to Twitter’s tweet-reply graph. It’s constantly changing. How could you apply this method (which currently is written for a static graph)?\n",
        "\n",
        "- How could you apply this to language (e.g. how could you treat language as a graph?)\n",
        "\n"
      ]
    }
  ]
}